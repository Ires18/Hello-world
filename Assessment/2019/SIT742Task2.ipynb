{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIT742Assignment2.ipynb",
      "provenance": []
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "version": "3.5.5",
      "name": "python",
      "file_extension": ".py",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "kernelspec": {
      "display_name": "Python 3.5",
      "name": "python3",
      "language": "python"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "_th7e2nu0At7"
      },
      "cell_type": "markdown",
      "source": [
        "# SIT742: Modern Data Science\n",
        "**(Assessment Task 02: Bank Marketing Data Analytics)**\n",
        "\n",
        "---\n",
        "- Materials in this module include resources collected from various open-source online repositories.\n",
        "- You are free to use, change and distribute this package.\n",
        "\n",
        "Prepared by **SIT742 Teaching Team**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Project Group Information:**\n",
        "\n",
        "- Names:\n",
        "- Student IDs:\n",
        "- Emails:\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "eASNvREtBU9G"
      },
      "cell_type": "markdown",
      "source": [
        "# 1.Import Spark"
      ]
    },
    {
      "metadata": {
        "id": "duCJfMrnGu00",
        "cellView": "code"
      },
      "cell_type": "code",
      "source": [
        "!pip install wget\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aGOo805LA-fM"
      },
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K7bEtO_fBZmE"
      },
      "cell_type": "markdown",
      "source": [
        "# 2.Read and check data"
      ]
    },
    {
      "metadata": {
        "id": "EADPspOv0Auh"
      },
      "cell_type": "code",
      "source": [
        "import wget\n",
        "link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Assessment/2019/data/bank.csv'\n",
        "DataSet = wget.download(link_to_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tiM-PiiR0Aup"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FQ8Ts9eZBA-M"
      },
      "cell_type": "code",
      "source": [
        "# Import the 'bank.csv' as a Spark dataframe and name it as df\n",
        "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
        "df = spark.read.csv('bank.csv', header = True, inferSchema = True)\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJGWVc0yB0UA"
      },
      "cell_type": "code",
      "source": [
        "# Check data distribution\n",
        "# You may use printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wKQMhrtFChHa"
      },
      "cell_type": "markdown",
      "source": [
        "# 3.Select features"
      ]
    },
    {
      "metadata": {
        "id": "VU5xMqN_RyM2"
      },
      "cell_type": "code",
      "source": [
        "#Select features ('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit') as df2\n",
        "df2=\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vwF5sqRYa_eI"
      },
      "cell_type": "code",
      "source": [
        "#Remove invalid rows/records using spark.sql"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A_T8qvxzR-oI"
      },
      "cell_type": "code",
      "source": [
        "#Covert categorical features to metric features using One hot encoding\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pOqHERYJCQuC"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 normalisation"
      ]
    },
    {
      "metadata": {
        "id": "cfL6_sca5VwI"
      },
      "cell_type": "code",
      "source": [
        "#Apply Min-Max normalisation on each attribute using MinMaxScaler\n",
        "from pyspark.ml.feature import MinMaxScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9trUY7ZgCzhW"
      },
      "cell_type": "markdown",
      "source": [
        "# 4.Unsupervised learning"
      ]
    },
    {
      "metadata": {
        "id": "KTQfUch2Cmmi"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.1 K-means"
      ]
    },
    {
      "metadata": {
        "id": "dGGZI70Ohqgg"
      },
      "cell_type": "code",
      "source": [
        "# Perform unsupervised learning on df2 with k-means\n",
        "# You can use whole df2 as both training and testing data,\n",
        "# Evaluate the clustering result using Accuracy.\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FHom8o2KCt36"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.2 PCA"
      ]
    },
    {
      "metadata": {
        "id": "wT4cx5uGTjmj"
      },
      "cell_type": "code",
      "source": [
        "#Generate a scatter plot using the first two PCA components to investigate the data distribution.\n",
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.ml.linalg import Vectors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ulp_uILXCv4Z"
      },
      "cell_type": "markdown",
      "source": [
        "# 5.Supervised learning"
      ]
    },
    {
      "metadata": {
        "id": "0O7tszcPfnHN"
      },
      "cell_type": "code",
      "source": [
        "train, test = df2.randomSplit([0.7, 0.3], seed = 742)\n",
        "print(\"Training Dataset Count: \" + str(train.count()))\n",
        "print(\"Test Dataset Count: \" + str(test.count()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2SsHdh7YC-eN"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1 LogisticRegression"
      ]
    },
    {
      "metadata": {
        "id": "Vqo_ywFQYxSj"
      },
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hANwFUzhgG83"
      },
      "cell_type": "code",
      "source": [
        "#Exam the coefficients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "evM5eiJoDHw2"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.2 Decision tree"
      ]
    },
    {
      "metadata": {
        "id": "He4mlHb7hBoY"
      },
      "cell_type": "code",
      "source": [
        "#Decision tree\n",
        "from pyspark.ml.classification import DecisionTreeClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CaE-Z_IlDKXF"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.3 NaiveBayes"
      ]
    },
    {
      "metadata": {
        "id": "v2XL6I0t7irt"
      },
      "cell_type": "code",
      "source": [
        "#NaiveBayes\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}